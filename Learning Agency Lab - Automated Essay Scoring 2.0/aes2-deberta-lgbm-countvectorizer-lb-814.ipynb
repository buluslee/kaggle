{
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "sourceId": 71485,
     "databundleVersionId": 8059942,
     "sourceType": "competition"
    },
    {
     "sourceId": 7017419,
     "sourceType": "datasetVersion",
     "datasetId": 3937250
    },
    {
     "sourceId": 8141507,
     "sourceType": "datasetVersion",
     "datasetId": 4813598
    },
    {
     "sourceId": 8166166,
     "sourceType": "datasetVersion",
     "datasetId": 4832208
    },
    {
     "sourceId": 8260229,
     "sourceType": "datasetVersion",
     "datasetId": 4902588
    },
    {
     "sourceId": 8358085,
     "sourceType": "datasetVersion",
     "datasetId": 4966859
    },
    {
     "sourceId": 8404441,
     "sourceType": "datasetVersion",
     "datasetId": 4956698
    },
    {
     "sourceId": 8404458,
     "sourceType": "datasetVersion",
     "datasetId": 5001044
    },
    {
     "sourceId": 8456403,
     "sourceType": "datasetVersion",
     "datasetId": 5040072
    },
    {
     "sourceId": 8704338,
     "sourceType": "datasetVersion",
     "datasetId": 5220886
    },
    {
     "sourceId": 8755287,
     "sourceType": "datasetVersion",
     "datasetId": 5259587
    },
    {
     "sourceId": 170434135,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 170531930,
     "sourceType": "kernelVersion"
    }
   ],
   "dockerImageVersionId": 30674,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## ℹ️ Info\n",
    "\n",
    "* **forked original great work kernels**\n",
    "    * https://www.kaggle.com/code/siddhvr/aes-2-0-deberta-lgbm-baseline\n",
    "    * https://www.kaggle.com/code/olyatsimboy/5-fold-deberta-lgbm\n",
    "    * https://www.kaggle.com/code/aikhmelnytskyy/quick-start-lgbm\n",
    "    \n",
    "\n",
    "* **2024/04/15 My Additional**\n",
    "    * add CountVectorizer FEs.\n",
    "    ```\n",
    "    vectorizer_cnt = CountVectorizer(\n",
    "            tokenizer=lambda x: x,\n",
    "            preprocessor=lambda x: x,\n",
    "            token_pattern=None,\n",
    "            strip_accents='unicode',\n",
    "            analyzer = 'word',\n",
    "            ngram_range=(2,3),\n",
    "            min_df=0.10,\n",
    "            max_df=0.85,\n",
    "    ```\n",
    "\n",
    "* **2024/04/21 My Additional**\n",
    "    * Add MetaFEs. Train deberta-v3-large local(5Fold SKF)\n",
    "    * https://www.kaggle.com/datasets/hideyukizushi/aes2-400-20240419134941\n",
    "    ```\n",
    "    # CV Result\n",
    "    [Fold0] 0.8358919189593519\n",
    "    [Fold1] 0.8363556728124214\n",
    "    [Fold2] 0.8180511976390897\n",
    "    [Fold3] 0.8332431669531231\n",
    "    [Fold4] 0.8331952936995911\n",
    "    ```\n",
    "\n",
    "\n",
    "* **2024/04/29 add these great kernel ideas**\n",
    "    * NewFE:count_spelling_errors\n",
    "    * newFE:remove_punctuation\n",
    "    * https://www.kaggle.com/code/xianhellg/more-feature-engineering-feature-selection-0-817https://www.kaggle.com/code/xianhellg/more-feature-engineering-feature-selection-0-817\n",
    "    ```\n",
    "    # Appendix.\n",
    "    \n",
    "    As a change from the original.\n",
    "    In my experiments, I extract features with particularly high importance.\n",
    "    /kaggle/input/aes2-400-fes-202404291649/usefe_list.pkl\n",
    "    \n",
    "    ```\n",
    "\n",
    "---"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "import gc\n",
    "import torch\n",
    "import copy\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments,DataCollatorWithPadding\n",
    "import nltk\n",
    "from datasets import Dataset\n",
    "from glob import glob\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import re\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from scipy.special import softmax\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, AdaBoostClassifier,GradientBoostingClassifier,BaggingClassifier\n",
    "from sklearn.linear_model import LogisticRegression, Perceptron\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import GaussianNB,MultinomialNB,ComplementNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn import tree\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from imblearn.ensemble import BalancedBaggingClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, f1_score\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from lightgbm import log_evaluation, early_stopping\n",
    "import lightgbm as lgb\n",
    "# nltk.download('wordnet')"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-27T06:38:54.406586Z",
     "iopub.execute_input": "2024-06-27T06:38:54.407373Z",
     "iopub.status.idle": "2024-06-27T06:39:13.665394Z",
     "shell.execute_reply.started": "2024-06-27T06:38:54.407339Z",
     "shell.execute_reply": "2024-06-27T06:39:13.664632Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# add new function\n",
    "add_model=0\n",
    "six_features=0\n",
    "threshold=0\n",
    "gkf=0\n",
    "gkf_smooth=0\n",
    "gkf_as_feature=1"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-27T06:39:13.667228Z",
     "iopub.execute_input": "2024-06-27T06:39:13.667876Z",
     "iopub.status.idle": "2024-06-27T06:39:13.6727Z",
     "shell.execute_reply.started": "2024-06-27T06:39:13.667821Z",
     "shell.execute_reply": "2024-06-27T06:39:13.671711Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "MAX_LENGTH = 1024\n",
    "TEST_DATA_PATH = \"/kaggle/input/learning-agency-lab-automated-essay-scoring-2/test.csv\"\n",
    "if add_model:\n",
    "    MODEL_PATH = [\n",
    "        '/kaggle/input/aes2-400-20240419134941/*/*',\n",
    "        '/kaggle/input/deberta-kaggle-0-815932/output/*/*'\n",
    "    ]\n",
    "else:\n",
    "    MODEL_PATH = '/kaggle/input/aes2-400-20240419134941/*/*'\n",
    "EVAL_BATCH_SIZE = 1"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-27T06:39:13.674196Z",
     "iopub.execute_input": "2024-06-27T06:39:13.674765Z",
     "iopub.status.idle": "2024-06-27T06:39:13.718684Z",
     "shell.execute_reply.started": "2024-06-27T06:39:13.674694Z",
     "shell.execute_reply": "2024-06-27T06:39:13.717789Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "if add_model:\n",
    "    models = []\n",
    "    for path in MODEL_PATH:\n",
    "        models.extend(glob(path))\n",
    "else:\n",
    "    models = glob(MODEL_PATH)\n",
    "tokenizer = AutoTokenizer.from_pretrained(models[0])\n",
    "\n",
    "def tokenize(sample):\n",
    "    return tokenizer(sample['full_text'], max_length=MAX_LENGTH, truncation=True)\n",
    "\n",
    "df_test = pd.read_csv(TEST_DATA_PATH)\n",
    "ds = Dataset.from_pandas(df_test).map(tokenize).remove_columns(['essay_id', 'full_text'])\n",
    "\n",
    "args = TrainingArguments(\n",
    "    \".\", \n",
    "    per_device_eval_batch_size=EVAL_BATCH_SIZE, \n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "predictions = []\n",
    "for model in models:\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model)\n",
    "    trainer = Trainer(\n",
    "        model=model, \n",
    "        args=args, \n",
    "        data_collator=DataCollatorWithPadding(tokenizer), \n",
    "        tokenizer=tokenizer\n",
    "    )    \n",
    "    preds = trainer.predict(ds).predictions\n",
    "    predictions.append(softmax(preds, axis=-1))\n",
    "    del model, trainer\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-27T06:39:13.719824Z",
     "iopub.execute_input": "2024-06-27T06:39:13.720119Z",
     "iopub.status.idle": "2024-06-27T06:40:19.650321Z",
     "shell.execute_reply.started": "2024-06-27T06:39:13.720095Z",
     "shell.execute_reply": "2024-06-27T06:40:19.649254Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "predicted_score = 0.\n",
    "for p in predictions:\n",
    "    predicted_score += p\n",
    "    \n",
    "predicted_score /= len(predictions)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-27T06:40:19.653045Z",
     "iopub.execute_input": "2024-06-27T06:40:19.653351Z",
     "iopub.status.idle": "2024-06-27T06:40:19.658467Z",
     "shell.execute_reply.started": "2024-06-27T06:40:19.653323Z",
     "shell.execute_reply": "2024-06-27T06:40:19.656906Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "df_test['score'] = predicted_score.argmax(-1) + 1\n",
    "df_test.head()"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-27T06:40:19.721551Z",
     "iopub.execute_input": "2024-06-27T06:40:19.721971Z",
     "iopub.status.idle": "2024-06-27T06:40:19.737819Z",
     "shell.execute_reply.started": "2024-06-27T06:40:19.721933Z",
     "shell.execute_reply": "2024-06-27T06:40:19.736993Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# **Data Loading**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "columns = [  \n",
    "    (\n",
    "        pl.col(\"full_text\").str.split(by=\"\\n\\n\").alias(\"paragraph\")\n",
    "    ),\n",
    "]\n",
    "PATH = \"/kaggle/input/learning-agency-lab-automated-essay-scoring-2/\"\n",
    "\n",
    "train = pl.read_csv(PATH + \"train.csv\").with_columns(columns)\n",
    "test = pl.read_csv(PATH + \"test.csv\").with_columns(columns)\n",
    "\n",
    "train.head(1)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-27T06:40:19.739033Z",
     "iopub.execute_input": "2024-06-27T06:40:19.739413Z",
     "iopub.status.idle": "2024-06-27T06:40:20.3473Z",
     "shell.execute_reply.started": "2024-06-27T06:40:19.739386Z",
     "shell.execute_reply": "2024-06-27T06:40:20.346399Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# **Preprocessing**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "cList = {\n",
    "  \"ain't\": \"am not\",\"aren't\": \"are not\",\"can't\": \"cannot\",\"can't've\": \"cannot have\",\"'cause\": \"because\",  \"could've\": \"could have\",\"couldn't\": \"could not\",\"couldn't've\": \"could not have\",\"didn't\": \"did not\",\"doesn't\": \"does not\",\"don't\": \"do not\",\"hadn't\": \"had not\",\"hadn't've\": \"had not have\",\"hasn't\": \"has not\",\n",
    "  \"haven't\": \"have not\",\"he'd\": \"he would\",\"he'd've\": \"he would have\",\"he'll\": \"he will\",\"he'll've\": \"he will have\",\"he's\": \"he is\",\n",
    "  \"how'd\": \"how did\",\"how'd'y\": \"how do you\",\"how'll\": \"how will\",\"how's\": \"how is\",\"I'd\": \"I would\",\"I'd've\": \"I would have\",\"I'll\": \"I will\",\"I'll've\": \"I will have\",\"I'm\": \"I am\",\"I've\": \"I have\",\n",
    "  \"isn't\": \"is not\",\"it'd\": \"it had\",\"it'd've\": \"it would have\",\"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\",\"let's\": \"let us\",\"ma'am\": \"madam\",\"mayn't\": \"may not\",\n",
    "  \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\",\"must've\": \"must have\",\"mustn't\": \"must not\",\"mustn't've\": \"must not have\",\"needn't\": \"need not\",\"needn't've\": \"need not have\",\"o'clock\": \"of the clock\",\"oughtn't\": \"ought not\",\"oughtn't've\": \"ought not have\",\"shan't\": \"shall not\",\"sha'n't\": \"shall not\",\n",
    "  \"shan't've\": \"shall not have\",\"she'd\": \"she would\",\"she'd've\": \"she would have\",\"she'll\": \"she will\",\"she'll've\": \"she will have\",\"she's\": \"she is\",\n",
    "  \"should've\": \"should have\",\"shouldn't\": \"should not\",\"shouldn't've\": \"should not have\",\"so've\": \"so have\",\"so's\": \"so is\",\"that'd\": \"that would\",\"that'd've\": \"that would have\",\"that's\": \"that is\",\"there'd\": \"there had\",\"there'd've\": \"there would have\",\"there's\": \"there is\",\"they'd\": \"they would\",\"they'd've\": \"they would have\",\"they'll\": \"they will\",\"they'll've\": \"they will have\",\"they're\": \"they are\",\"they've\": \"they have\",\"to've\": \"to have\",\"wasn't\": \"was not\",\"we'd\": \"we had\",\n",
    "  \"we'd've\": \"we would have\",\"we'll\": \"we will\",\"we'll've\": \"we will have\",\"we're\": \"we are\",\"we've\": \"we have\",\n",
    "  \"weren't\": \"were not\",\"what'll\": \"what will\",\"what'll've\": \"what will have\",\n",
    "  \"what're\": \"what are\",\"what's\": \"what is\",\"what've\": \"what have\",\"when's\": \"when is\",\"when've\": \"when have\",\n",
    "  \"where'd\": \"where did\",\"where's\": \"where is\",\"where've\": \"where have\",\"who'll\": \"who will\",\"who'll've\": \"who will have\",\"who's\": \"who is\",\"who've\": \"who have\",\"why's\": \"why is\",\n",
    "  \"why've\": \"why have\",\"will've\": \"will have\",\"won't\": \"will not\",\"won't've\": \"will not have\",\"would've\": \"would have\",\"wouldn't\": \"would not\",\n",
    "  \"wouldn't've\": \"would not have\",\"y'all\": \"you all\",\"y'alls\": \"you alls\",\"y'all'd\": \"you all would\",\n",
    "  \"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\"you'd\": \"you had\",\"you'd've\": \"you would have\",\"you'll\": \"you you will\",\"you'll've\": \"you you will have\",\"you're\": \"you are\",  \"you've\": \"you have\"\n",
    "   }\n",
    "\n",
    "c_re = re.compile('(%s)' % '|'.join(cList.keys()))\n",
    "\n",
    "def expandContractions(text, c_re=c_re):\n",
    "    def replace(match):\n",
    "        return cList[match.group(0)]\n",
    "    return c_re.sub(replace, text)\n",
    "\n",
    "def removeHTML(x):\n",
    "    html=re.compile(r'<.*?>')\n",
    "    return html.sub(r'',x)\n",
    "def dataPreprocessing(x):\n",
    "    x = x.lower()\n",
    "    x = removeHTML(x)\n",
    "    x = re.sub(\"@\\w+\", '',x)\n",
    "    x = re.sub(\"'\\d+\", '',x)\n",
    "    x = re.sub(\"\\d+\", '',x)\n",
    "    x = re.sub(\"http\\w+\", '',x)\n",
    "    x = re.sub(r\"\\s+\", \" \", x)\n",
    "#     x = expandContractions(x)\n",
    "    x = re.sub(r\"\\.+\", \".\", x)\n",
    "    x = re.sub(r\"\\,+\", \",\", x)\n",
    "    x = x.strip()\n",
    "    return x"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-27T06:40:20.348835Z",
     "iopub.execute_input": "2024-06-27T06:40:20.349205Z",
     "iopub.status.idle": "2024-06-27T06:40:20.370076Z",
     "shell.execute_reply.started": "2024-06-27T06:40:20.349173Z",
     "shell.execute_reply": "2024-06-27T06:40:20.369115Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# **Feature Engineering**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "import spacy\n",
    "import re\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "with open('/kaggle/input/english-word-hx/words.txt', 'r') as file:\n",
    "    english_vocab = set(word.strip().lower() for word in file)\n",
    "    \n",
    "def count_spelling_errors(text):\n",
    "    doc = nlp(text)\n",
    "    lemmatized_tokens = [token.lemma_.lower() for token in doc]\n",
    "    spelling_errors = sum(1 for token in lemmatized_tokens if token not in english_vocab)\n",
    "    return spelling_errors"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-27T06:40:20.371374Z",
     "iopub.execute_input": "2024-06-27T06:40:20.371698Z",
     "iopub.status.idle": "2024-06-27T06:40:22.759593Z",
     "shell.execute_reply.started": "2024-06-27T06:40:20.371668Z",
     "shell.execute_reply": "2024-06-27T06:40:22.758565Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import string\n",
    "def remove_punctuation(text):\n",
    "    \"\"\"\n",
    "    Remove all punctuation from the input text.\n",
    "    \n",
    "    Args:\n",
    "    - text (str): The input text.\n",
    "    \n",
    "    Returns:\n",
    "    - str: The text with punctuation removed.\n",
    "    \"\"\"\n",
    "\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    return text.translate(translator)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-27T06:40:22.760932Z",
     "iopub.execute_input": "2024-06-27T06:40:22.761298Z",
     "iopub.status.idle": "2024-06-27T06:40:22.767493Z",
     "shell.execute_reply.started": "2024-06-27T06:40:22.761264Z",
     "shell.execute_reply": "2024-06-27T06:40:22.766528Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.Paragraph Features"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "def Paragraph_Preprocess(tmp):\n",
    "\n",
    "    tmp = tmp.explode('paragraph')\n",
    "    tmp = tmp.with_columns(pl.col('paragraph').map_elements(dataPreprocessing))\n",
    "    tmp = tmp.with_columns(pl.col('paragraph').map_elements(remove_punctuation).alias('paragraph_no_pinctuation'))\n",
    "    tmp = tmp.with_columns(pl.col('paragraph_no_pinctuation').map_elements(count_spelling_errors).alias(\"paragraph_error_num\"))\n",
    "    tmp = tmp.with_columns(pl.col('paragraph').map_elements(lambda x: len(x)).alias(\"paragraph_len\"))\n",
    "    tmp = tmp.with_columns(pl.col('paragraph').map_elements(lambda x: len(x.split('.'))).alias(\"paragraph_sentence_cnt\"),\n",
    "                    pl.col('paragraph').map_elements(lambda x: len(x.split(' '))).alias(\"paragraph_word_cnt\"),)\n",
    "    return tmp\n",
    "\n",
    "# feature_eng\n",
    "paragraph_fea = ['paragraph_len','paragraph_sentence_cnt','paragraph_word_cnt']\n",
    "paragraph_fea2 = ['paragraph_error_num'] + paragraph_fea\n",
    "def Paragraph_Eng(train_tmp):\n",
    "    num_list = [0, 50,75,100,125,150,175,200,250,300,350,400,500,600]\n",
    "    num_list2 = [0, 50,75,100,125,150,175,200,250,300,350,400,500,600,700]\n",
    "    aggs = [\n",
    "        *[pl.col('paragraph').filter(pl.col('paragraph_len') >= i).count().alias(f\"paragraph_>{i}_cnt\") for i in [0, 50,75,100,125,150,175,200,250,300,350,400,500,600,700] ], \n",
    "        *[pl.col('paragraph').filter(pl.col('paragraph_len') <= i).count().alias(f\"paragraph_<{i}_cnt\") for i in [25,49]], \n",
    "        *[pl.col(fea).max().alias(f\"{fea}_max\") for fea in paragraph_fea2],\n",
    "        *[pl.col(fea).mean().alias(f\"{fea}_mean\") for fea in paragraph_fea2],\n",
    "        *[pl.col(fea).min().alias(f\"{fea}_min\") for fea in paragraph_fea2],\n",
    "        *[pl.col(fea).sum().alias(f\"{fea}_sum\") for fea in paragraph_fea2],\n",
    "        *[pl.col(fea).first().alias(f\"{fea}_first\") for fea in paragraph_fea2],\n",
    "        *[pl.col(fea).last().alias(f\"{fea}_last\") for fea in paragraph_fea2],\n",
    "        *[pl.col(fea).kurtosis().alias(f\"{fea}_kurtosis\") for fea in paragraph_fea2],\n",
    "        *[pl.col(fea).quantile(0.25).alias(f\"{fea}_q1\") for fea in paragraph_fea2],\n",
    "        *[pl.col(fea).quantile(0.75).alias(f\"{fea}_q3\") for fea in paragraph_fea2],\n",
    "        ]\n",
    "    df = train_tmp.group_by(['essay_id'], maintain_order=True).agg(aggs).sort(\"essay_id\")\n",
    "    df = df.to_pandas()\n",
    "    return df\n",
    "tmp = Paragraph_Preprocess(train)\n",
    "train_feats = Paragraph_Eng(tmp)\n",
    "train_feats['score'] = train['score']\n",
    "\n",
    "feature_names = list(filter(lambda x: x not in ['essay_id','score'], train_feats.columns))\n",
    "print('Features Number: ',len(feature_names))\n",
    "train_feats.head(3)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-27T06:40:22.768675Z",
     "iopub.execute_input": "2024-06-27T06:40:22.769065Z",
     "iopub.status.idle": "2024-06-27T07:01:17.622659Z",
     "shell.execute_reply.started": "2024-06-27T06:40:22.769033Z",
     "shell.execute_reply": "2024-06-27T07:01:17.621724Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.Sentence Features"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "def Sentence_Preprocess(tmp):\n",
    "    tmp = tmp.with_columns(pl.col('full_text').map_elements(dataPreprocessing).str.split(by=\".\").alias(\"sentence\"))\n",
    "    tmp = tmp.explode('sentence')\n",
    "    tmp = tmp.with_columns(pl.col('sentence').map_elements(lambda x: len(x)).alias(\"sentence_len\"))\n",
    "    tmp = tmp.with_columns(pl.col('sentence').map_elements(lambda x: len(x.split(' '))).alias(\"sentence_word_cnt\"))    \n",
    "    return tmp\n",
    "\n",
    "# feature_eng\n",
    "sentence_fea = ['sentence_len','sentence_word_cnt']\n",
    "def Sentence_Eng(train_tmp):\n",
    "    aggs = [\n",
    "        *[pl.col('sentence').filter(pl.col('sentence_len') >= i).count().alias(f\"sentence_>{i}_cnt\") for i in [0,15,50,100,150,200,250,300] ], \n",
    "        *[pl.col('sentence').filter(pl.col('sentence_len') <= i).count().alias(f\"sentence_<{i}_cnt\") for i in [15,50] ], \n",
    "        *[pl.col(fea).max().alias(f\"{fea}_max\") for fea in sentence_fea],\n",
    "        *[pl.col(fea).mean().alias(f\"{fea}_mean\") for fea in sentence_fea],\n",
    "        *[pl.col(fea).min().alias(f\"{fea}_min\") for fea in sentence_fea],\n",
    "        *[pl.col(fea).sum().alias(f\"{fea}_sum\") for fea in sentence_fea],\n",
    "        *[pl.col(fea).first().alias(f\"{fea}_first\") for fea in sentence_fea],\n",
    "        *[pl.col(fea).last().alias(f\"{fea}_last\") for fea in sentence_fea],\n",
    "        *[pl.col(fea).kurtosis().alias(f\"{fea}_kurtosis\") for fea in sentence_fea],\n",
    "        *[pl.col(fea).quantile(0.25).alias(f\"{fea}_q1\") for fea in sentence_fea],\n",
    "        *[pl.col(fea).quantile(0.75).alias(f\"{fea}_q3\") for fea in sentence_fea],\n",
    "    \n",
    "        ]\n",
    "    df = train_tmp.group_by(['essay_id'], maintain_order=True).agg(aggs).sort(\"essay_id\")\n",
    "    df = df.to_pandas()\n",
    "    return df\n",
    "\n",
    "tmp = Sentence_Preprocess(train)\n",
    "train_feats = train_feats.merge(Sentence_Eng(tmp), on='essay_id', how='left')\n",
    "\n",
    "feature_names = list(filter(lambda x: x not in ['essay_id','score'], train_feats.columns))\n",
    "print('Features Number: ',len(feature_names))\n",
    "train_feats.head(3)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-27T07:01:17.624104Z",
     "iopub.execute_input": "2024-06-27T07:01:17.624469Z",
     "iopub.status.idle": "2024-06-27T07:01:24.57974Z",
     "shell.execute_reply.started": "2024-06-27T07:01:17.624434Z",
     "shell.execute_reply": "2024-06-27T07:01:24.578677Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.Word Features"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# word feature\n",
    "def Word_Preprocess(tmp):\n",
    "    tmp = tmp.with_columns(pl.col('full_text').map_elements(dataPreprocessing).str.split(by=\" \").alias(\"word\"))\n",
    "    tmp = tmp.explode('word')\n",
    "    tmp = tmp.with_columns(pl.col('word').map_elements(lambda x: len(x)).alias(\"word_len\"))\n",
    "    tmp = tmp.filter(pl.col('word_len')!=0)    \n",
    "    return tmp\n",
    "\n",
    "# feature_eng\n",
    "def Word_Eng(train_tmp):\n",
    "    aggs = [\n",
    "        *[pl.col('word').filter(pl.col('word_len') >= i+1).count().alias(f\"word_{i+1}_cnt\") for i in range(15) ], \n",
    "        pl.col('word_len').max().alias(f\"word_len_max\"),\n",
    "        pl.col('word_len').mean().alias(f\"word_len_mean\"),\n",
    "        pl.col('word_len').std().alias(f\"word_len_std\"),\n",
    "        pl.col('word_len').quantile(0.25).alias(f\"word_len_q1\"),\n",
    "        pl.col('word_len').quantile(0.50).alias(f\"word_len_q2\"),\n",
    "        pl.col('word_len').quantile(0.75).alias(f\"word_len_q3\"),\n",
    "        ]\n",
    "    df = train_tmp.group_by(['essay_id'], maintain_order=True).agg(aggs).sort(\"essay_id\")\n",
    "    df = df.to_pandas()\n",
    "    return df\n",
    "\n",
    "tmp = Word_Preprocess(train)\n",
    "train_feats = train_feats.merge(Word_Eng(tmp), on='essay_id', how='left')\n",
    "\n",
    "feature_names = list(filter(lambda x: x not in ['essay_id','score'], train_feats.columns))\n",
    "print('Features Number: ',len(feature_names))\n",
    "train_feats.head(3)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-27T07:01:24.580894Z",
     "iopub.execute_input": "2024-06-27T07:01:24.58119Z",
     "iopub.status.idle": "2024-06-27T07:01:37.594728Z",
     "shell.execute_reply.started": "2024-06-27T07:01:24.581165Z",
     "shell.execute_reply": "2024-06-27T07:01:37.593865Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 4.Tf-idf features"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "vectorizer = TfidfVectorizer(\n",
    "            tokenizer=lambda x: x,\n",
    "            preprocessor=lambda x: x,\n",
    "            token_pattern=None,\n",
    "            strip_accents='unicode',\n",
    "            analyzer = 'word',\n",
    "            ngram_range=(3,6),\n",
    "            min_df=0.05,\n",
    "            max_df=0.95,\n",
    "            sublinear_tf=True,\n",
    ")\n",
    "\n",
    "train_tfid = vectorizer.fit_transform([i for i in train['full_text']])\n",
    "dense_matrix = train_tfid.toarray()\n",
    "df = pd.DataFrame(dense_matrix)\n",
    "tfid_columns = [ f'tfid_{i}' for i in range(len(df.columns))]\n",
    "df.columns = tfid_columns\n",
    "df['essay_id'] = train_feats['essay_id']\n",
    "train_feats = train_feats.merge(df, on='essay_id', how='left')\n",
    "feature_names = list(filter(lambda x: x not in ['essay_id','score'], train_feats.columns))\n",
    "print('Number of Features: ',len(feature_names))\n",
    "train_feats.head(3)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-27T07:01:37.598654Z",
     "iopub.execute_input": "2024-06-27T07:01:37.598972Z",
     "iopub.status.idle": "2024-06-27T07:04:52.078985Z",
     "shell.execute_reply.started": "2024-06-27T07:01:37.598947Z",
     "shell.execute_reply": "2024-06-27T07:04:52.077971Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 5.CountVectorizer Features"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "vectorizer_cnt = CountVectorizer(\n",
    "            tokenizer=lambda x: x,\n",
    "            preprocessor=lambda x: x,\n",
    "            token_pattern=None,\n",
    "            strip_accents='unicode',\n",
    "            analyzer = 'word',\n",
    "            ngram_range=(2,3),\n",
    "            min_df=0.10,\n",
    "            max_df=0.85,\n",
    ")\n",
    "train_tfid = vectorizer_cnt.fit_transform([i for i in train['full_text']])\n",
    "dense_matrix = train_tfid.toarray()\n",
    "df = pd.DataFrame(dense_matrix)\n",
    "tfid_columns = [ f'tfid_cnt_{i}' for i in range(len(df.columns))]\n",
    "df.columns = tfid_columns\n",
    "df['essay_id'] = train_feats['essay_id']\n",
    "train_feats = train_feats.merge(df, on='essay_id', how='left')"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-27T07:04:52.08028Z",
     "iopub.execute_input": "2024-06-27T07:04:52.080644Z",
     "iopub.status.idle": "2024-06-27T07:06:06.9451Z",
     "shell.execute_reply.started": "2024-06-27T07:04:52.080611Z",
     "shell.execute_reply": "2024-06-27T07:06:06.944137Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 6.Six-features"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "if six_features:\n",
    "    !pip install --no-index --no-deps /kaggle/input/aes-whls/aes_whls/sentence_transformers-2.8.0.dev0-py3-none-any.whl\n",
    "    train_f = pd.read_csv(PATH + \"train.csv\")\n",
    "    test_f = pd.read_csv(PATH + \"test.csv\")\n",
    "    from sentence_transformers import SentenceTransformer, models\n",
    "\n",
    "    feedback_df = pd.read_csv('/kaggle/input/feedback-data/feedback_data.csv')\n",
    "\n",
    "    feed_embeds = []\n",
    "\n",
    "    merged_embeds = []\n",
    "\n",
    "    test_embeds = []\n",
    "\n",
    "    for i in range(5):\n",
    "        model_path = f'/kaggle/input/sent-debsmall/deberta_small_trained/temp_fold{i}_checkpoints'\n",
    "        word_embedding_model = models.Transformer(model_path, max_seq_length=1024)\n",
    "        pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension())\n",
    "        model = SentenceTransformer(modules=[word_embedding_model, pooling_model])\n",
    "\n",
    "        model.half()\n",
    "        model = model.to('cuda')\n",
    "\n",
    "        feed_custom_embeddings_train = model.encode(feedback_df.loc[:, 'full_text'].values, device='cuda',\n",
    "                                                    show_progress_bar=True, normalize_embeddings=True)\n",
    "\n",
    "        feed_embeds.append(feed_custom_embeddings_train)\n",
    "\n",
    "        merged_custom_embeddings = model.encode(train_f.loc[:, 'full_text'].values, device='cuda',\n",
    "                                                show_progress_bar=True, normalize_embeddings=True)\n",
    "\n",
    "        merged_embeds.append(merged_custom_embeddings)\n",
    "\n",
    "\n",
    "        test_custom_embeddings = model.encode(test_f.loc[:, 'full_text'].values, device='cuda',\n",
    "                                                show_progress_bar=True, normalize_embeddings=True)\n",
    "\n",
    "        test_embeds.append(test_custom_embeddings)\n",
    "\n",
    "    feed_embeds = np.mean(feed_embeds, axis=0)\n",
    "    merged_embeds = np.mean(merged_embeds, axis=0)\n",
    "    test_embeds = np.mean(test_embeds, axis=0)\n",
    "\n",
    "    from sklearn.linear_model import Ridge\n",
    "    from sklearn.multioutput import MultiOutputRegressor\n",
    "\n",
    "    targets = ['cohesion', 'syntax', 'vocabulary', 'phraseology', 'grammar', 'conventions']\n",
    "\n",
    "\n",
    "    ridge = Ridge(alpha=1.0)\n",
    "\n",
    "    multioutputregressor = MultiOutputRegressor(ridge)\n",
    "\n",
    "\n",
    "\n",
    "    multioutputregressor.fit(feed_embeds, feedback_df.loc[:, targets])\n",
    "\n",
    "    feedback_predictions = multioutputregressor.predict(merged_embeds)\n",
    "\n",
    "    feedback_predictions_df = pd.DataFrame(feedback_predictions, columns=targets)\n",
    "\n",
    "    test_feedback_predictions = multioutputregressor.predict(test_embeds)\n",
    "\n",
    "    test_feedback_predictions_df = pd.DataFrame(test_feedback_predictions, columns=targets)\n",
    "\n",
    "    print(feedback_predictions_df)# .head()\n",
    "\n",
    "    train_feats = pd.concat((train_feats, feedback_predictions_df), axis=1)\n",
    "    \n",
    "train_feats.shape"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-27T07:06:06.946311Z",
     "iopub.execute_input": "2024-06-27T07:06:06.946607Z",
     "iopub.status.idle": "2024-06-27T07:06:06.969709Z",
     "shell.execute_reply.started": "2024-06-27T07:06:06.946582Z",
     "shell.execute_reply": "2024-06-27T07:06:06.968813Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 7.Meta Features(deberta-v3-large)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "import joblib\n",
    "\n",
    "deberta_oof = joblib.load('/kaggle/input/aes2-400-20240419134941/oof.pkl')\n",
    "print(deberta_oof.shape, train_feats.shape)\n",
    "\n",
    "for i in range(6):\n",
    "    train_feats[f'deberta_oof_{i}'] = deberta_oof[:, i]\n",
    "\n",
    "feature_names = list(filter(lambda x: x not in ['essay_id','score'], train_feats.columns))\n",
    "print('Features Number: ',len(feature_names))    \n",
    "\n",
    "train_feats.shape"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-27T07:06:06.970934Z",
     "iopub.execute_input": "2024-06-27T07:06:06.971262Z",
     "iopub.status.idle": "2024-06-27T07:06:07.025644Z",
     "shell.execute_reply.started": "2024-06-27T07:06:06.971232Z",
     "shell.execute_reply": "2024-06-27T07:06:07.024674Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 7.prompt Features"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "gkf_train_df = pd.read_csv(\"/kaggle/input/learning-agency-lab-automated-essay-scoring-2/train.csv\")\n",
    "persuade = pd.read_csv(\"/kaggle/input/persaude-corpus-2/persuade_2.0_human_scores_demo_id_github.csv\")\n",
    "\n",
    "intersection = pd.merge(gkf_train_df, persuade, on=\"full_text\", how=\"inner\")[[\"essay_id\", \"full_text\", \"prompt_name\"]].reset_index(drop=True)\n",
    "difference = gkf_train_df[~gkf_train_df[\"essay_id\"].isin(intersection[\"essay_id\"])].reset_index(drop=True)\n",
    "\n",
    "intersection[\"label\"] =  intersection[\"prompt_name\"].astype(\"category\").cat.codes\n",
    "intersection.head()\n",
    "\n",
    "prompt_to_id = intersection.drop_duplicates(subset=(\"prompt_name\", \"label\"))[[\"prompt_name\", \"label\"]]\n",
    "label2id = {row[\"prompt_name\"]: row[\"label\"] for _, row in prompt_to_id.iterrows()}\n",
    "id2label = {row[\"label\"]: row[\"prompt_name\"] for _, row in prompt_to_id.iterrows()}\n",
    "\n",
    "\n",
    "def get_prompt(gkf_df,df,tokenizer):\n",
    "    \n",
    "    intersection = pd.merge(gkf_df, persuade, on=\"full_text\", how=\"inner\")[[\"essay_id\", \"full_text\", \"prompt_name\"]].reset_index(drop=True)\n",
    "    difference = gkf_df[~gkf_df[\"essay_id\"].isin(intersection[\"essay_id\"])].reset_index(drop=True)\n",
    "    print(\"len(intersection):\", len(intersection))\n",
    "    print(\"len(difference):\", len(difference))\n",
    "    \n",
    "    \n",
    "    intersection_ds = Dataset.from_pandas(intersection)\n",
    "    diff_ds = Dataset.from_pandas(difference)\n",
    "    intersection_ds = intersection_ds.map(lambda i: tokenizer(i[\"full_text\"], max_length=1024, truncation=True), batched=True)\n",
    "    diff_ds = diff_ds.map(lambda i: tokenizer(i[\"full_text\"], max_length=1024, truncation=True), batched=True)\n",
    "    \n",
    "    MODEL_PATHS_PROMPT = [\n",
    "        '/kaggle/input/aes-train-prompt-added-csv/output/*/*'\n",
    "    ]\n",
    "    EVAL_BATCH_SIZE = 1\n",
    "\n",
    "    model_pro = []\n",
    "    for path in MODEL_PATHS_PROMPT:\n",
    "        model_pro.extend(glob(path))\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_pro[0])\n",
    "\n",
    "    def tokenize(sample):\n",
    "        return tokenizer(sample['full_text'], max_length=MAX_LENGTH, truncation=True)\n",
    "\n",
    "    df_test = pd.read_csv(TEST_DATA_PATH)\n",
    "    ds = Dataset.from_pandas(df_test).map(tokenize).remove_columns(['essay_id', 'full_text'])\n",
    "\n",
    "    args = TrainingArguments(\n",
    "        \".\", \n",
    "        per_device_eval_batch_size=EVAL_BATCH_SIZE, \n",
    "        report_to=\"none\"\n",
    "    )\n",
    "    \n",
    "    if len(difference)!=0:\n",
    "        predictions = []\n",
    "        for model in model_pro:\n",
    "            model = AutoModelForSequenceClassification.from_pretrained(model)\n",
    "            trainer = Trainer(\n",
    "                model=model, \n",
    "                args=args, \n",
    "                data_collator=DataCollatorWithPadding(tokenizer), \n",
    "                tokenizer=tokenizer\n",
    "            )    \n",
    "            preds = trainer.predict(diff_ds).predictions\n",
    "            predictions.append(softmax(preds, axis=-1))\n",
    "            del model, trainer\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "\n",
    "        predicted_score = 0.\n",
    "        for p in predictions:\n",
    "            predicted_score += p\n",
    "\n",
    "        predicted_score /= len(predictions)\n",
    "\n",
    "        \n",
    "    if len(difference)!=0:\n",
    "        predictions=np.array(predicted_score)\n",
    "        predicted_score=dict()\n",
    "        for i in range(7):\n",
    "            predicted_score[f'prompt_{i}'] = predictions[:, i]\n",
    "\n",
    "        essay_id = diff_ds[\"essay_id\"]\n",
    "        result_df = pd.DataFrame({\"essay_id\": essay_id})\n",
    "\n",
    "        for i in range(7):\n",
    "            result_df[f\"prompt_{i}\"] = predicted_score[f\"prompt_{i}\"]\n",
    "\n",
    "\n",
    "    if len(intersection)!=0:\n",
    "        prompt_id = [label2id[i] for i in intersection[\"prompt_name\"]]\n",
    "        intersection['prompt_id']=prompt_id\n",
    "\n",
    "        intersection_df = intersection[[\"essay_id\"]].copy()\n",
    "        for i in range(7):\n",
    "            prompt_i=[1 if abs(i-j)<0.01 else 0 for j in intersection['prompt_id']]\n",
    "            intersection_df[f\"prompt_{i}\"]=prompt_i\n",
    "\n",
    "\n",
    "\n",
    "    if len(difference)!=0 and len(intersection)!=0:\n",
    "        final_df = pd.concat([result_df, intersection_df])\n",
    "    elif len(difference)!=0 and len(intersection)==0:\n",
    "        final_df=result_df\n",
    "    else:\n",
    "        final_df=intersection_df\n",
    "\n",
    "    prompts_df=pd.merge(df,final_df,on=\"essay_id\")\n",
    "\n",
    "    return prompts_df\n",
    "\n",
    "\n",
    "if gkf_as_feature:\n",
    "    gkf_train_df = pd.read_csv(\"/kaggle/input/learning-agency-lab-automated-essay-scoring-2/train.csv\")\n",
    "    gkf_test_df = pd.read_csv(\"/kaggle/input/learning-agency-lab-automated-essay-scoring-2/test.csv\")\n",
    "    df_train=pd.read_csv(\"/kaggle/input/learning-agency-lab-automated-essay-scoring-2/train.csv\")\n",
    "    train_prompts_df=get_prompt(gkf_train_df,df_train,tokenizer)\n",
    "    print(train_prompts_df.head(3))\n",
    "    test_prompts_df=get_prompt(gkf_test_df,df_test,tokenizer)\n",
    "    print(test_prompts_df.head(3))"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-27T07:06:07.027111Z",
     "iopub.execute_input": "2024-06-27T07:06:07.027458Z",
     "iopub.status.idle": "2024-06-27T07:09:54.355242Z",
     "shell.execute_reply.started": "2024-06-27T07:06:07.027428Z",
     "shell.execute_reply": "2024-06-27T07:09:54.354202Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "if gkf_as_feature:\n",
    "    for i in range(7):\n",
    "        train_feats[f'prompt_{i}'] = train_prompts_df[f'prompt_{i}']\n",
    "\n",
    "train_feats.shape"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-27T07:09:54.356422Z",
     "iopub.execute_input": "2024-06-27T07:09:54.35671Z",
     "iopub.status.idle": "2024-06-27T07:09:54.382595Z",
     "shell.execute_reply.started": "2024-06-27T07:09:54.356684Z",
     "shell.execute_reply": "2024-06-27T07:09:54.381666Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# **Model training**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "def quadratic_weighted_kappa(y_true, y_pred):\n",
    "    y_true = y_true + a\n",
    "    y_pred = (y_pred + a).clip(1, 6).round()\n",
    "    qwk = cohen_kappa_score(y_true, y_pred, weights=\"quadratic\")\n",
    "    return 'QWK', qwk, True\n",
    "def qwk_obj(y_true, y_pred):\n",
    "    labels = y_true + a\n",
    "    preds = y_pred + a\n",
    "    preds = preds.clip(1, 6)\n",
    "    f = 1/2*np.sum((preds-labels)**2)\n",
    "    g = 1/2*np.sum((preds-a)**2+b)\n",
    "    df = preds - labels\n",
    "    dg = preds - a\n",
    "    grad = (df/g - f*dg/g**2)*len(labels)\n",
    "    hess = np.ones(len(labels))\n",
    "    return grad, hess\n",
    "a = 2.998\n",
    "b = 1.092"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-27T07:09:54.383829Z",
     "iopub.execute_input": "2024-06-27T07:09:54.384585Z",
     "iopub.status.idle": "2024-06-27T07:09:54.392711Z",
     "shell.execute_reply.started": "2024-06-27T07:09:54.384552Z",
     "shell.execute_reply": "2024-06-27T07:09:54.391818Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# **FeatureSelection(Load High importance FE)**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "import pickle\n",
    "with open('/kaggle/input/aes2-400-fes-202404291649/usefe_list.pkl', mode='br') as fi:\n",
    "  feature_names = pickle.load(fi)\n",
    "if six_features:\n",
    "    feature_select = feature_names\n",
    "    six_features=['cohesion','syntax','vocabulary','phraseology','grammar','conventions']  # add\n",
    "    feature_select=feature_select+six_features\n",
    "elif gkf_as_feature:\n",
    "    feature_select = feature_names\n",
    "    prompt_features=['prompt_0','prompt_1','prompt_2','prompt_3','prompt_4','prompt_5','prompt_6']  # add\n",
    "    feature_select=feature_select+prompt_features\n",
    "else:\n",
    "    feature_select = feature_names"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-27T07:09:54.394226Z",
     "iopub.execute_input": "2024-06-27T07:09:54.394586Z",
     "iopub.status.idle": "2024-06-27T07:09:54.412391Z",
     "shell.execute_reply.started": "2024-06-27T07:09:54.394556Z",
     "shell.execute_reply": "2024-06-27T07:09:54.411626Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "X = train_feats[feature_select].astype(np.float32).values\n",
    "\n",
    "y_split = train_feats['score'].astype(int).values\n",
    "y = train_feats['score'].astype(np.float32).values-a\n",
    "if threshold:\n",
    "    oof = np.zeros(len(train), dtype='float32')\n",
    "    x_score_thres = train_feats['score'].astype(int).values  # add\n",
    "else:\n",
    "    oof = train_feats['score'].astype(int).values\n"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-27T07:09:54.413443Z",
     "iopub.execute_input": "2024-06-27T07:09:54.413784Z",
     "iopub.status.idle": "2024-06-27T07:09:55.625409Z",
     "shell.execute_reply.started": "2024-06-27T07:09:54.413749Z",
     "shell.execute_reply": "2024-06-27T07:09:55.624628Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "len(feature_select)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-27T07:09:55.626569Z",
     "iopub.execute_input": "2024-06-27T07:09:55.626873Z",
     "iopub.status.idle": "2024-06-27T07:09:55.632255Z",
     "shell.execute_reply.started": "2024-06-27T07:09:55.626836Z",
     "shell.execute_reply": "2024-06-27T07:09:55.631325Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# **Let's use cross-validation**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "if gkf:\n",
    "    n_splits = 7 \n",
    "    \n",
    "    from sklearn.model_selection import GroupKFold\n",
    "    gkf=GroupKFold(n_splits=7)\n",
    "    prompts_df=pd.read_csv(\"/kaggle/input/prompt-df/prompt_df.csv\")\n",
    "    preds_valid1=np.zeros(len(prompts_df))\n",
    "    \n",
    "    f1_scores = []\n",
    "    kappa_scores = []\n",
    "    models = []\n",
    "    predictions = []\n",
    "    callbacks = [log_evaluation(period=64), early_stopping(stopping_rounds=256,first_metric_only=True)]\n",
    "    \n",
    "    i=1\n",
    "    for train_index, test_index in gkf.split(prompts_df,prompts_df[\"score\"],groups=prompts_df[\"prompt_name\"]):\n",
    "\n",
    "        print('fold',i)\n",
    "        X_train_fold, X_test_fold = X[train_index], X[test_index]\n",
    "\n",
    "\n",
    "        y_train_fold, y_test_fold, y_test_fold_int = y[train_index], y[test_index], y_split[test_index]\n",
    "\n",
    "        model = lgb.LGBMRegressor(\n",
    "                    objective = qwk_obj,\n",
    "                    metrics = 'None',\n",
    "                    learning_rate = 0.1,     # 0.05\n",
    "                    max_depth = 8,           # 5\n",
    "                    num_leaves = 10,         # 10\n",
    "                    colsample_bytree=0.5,    # 0.3\n",
    "                    reg_alpha = 0.1,         # 0.7\n",
    "                    reg_lambda = 0.8,        # 0.1\n",
    "                    n_estimators=1024,        # 700\n",
    "                    random_state=42,         # 42\n",
    "                    extra_trees=True,\n",
    "                    class_weight='balanced',\n",
    "                    verbosity = - 1)\n",
    "    \n",
    "        predictor = model.fit(X_train_fold,\n",
    "                                      y_train_fold,\n",
    "                                      eval_names=['train', 'valid'],\n",
    "                                      eval_set=[(X_train_fold, y_train_fold), (X_test_fold, y_test_fold)],\n",
    "                                      eval_metric=quadratic_weighted_kappa,\n",
    "                                      callbacks=callbacks,)\n",
    "        models.append(predictor)\n",
    "        predictions_fold = predictor.predict(X_test_fold)\n",
    "        predictions_fold = predictions_fold + a\n",
    "        oof[test_index]=predictions_fold\n",
    "        predictions_fold = predictions_fold.clip(1, 6).round()\n",
    "        predictions.append(predictions_fold)\n",
    "        f1_fold = f1_score(y_test_fold_int, predictions_fold, average='weighted')\n",
    "        f1_scores.append(f1_fold)\n",
    "\n",
    "\n",
    "        kappa_fold = cohen_kappa_score(y_test_fold_int, predictions_fold, weights='quadratic')\n",
    "        kappa_scores.append(kappa_fold)\n",
    "\n",
    "        cm = confusion_matrix(y_test_fold_int, predictions_fold, labels=[x for x in range(1,7)])\n",
    "    \n",
    "        disp = ConfusionMatrixDisplay(confusion_matrix=cm,\n",
    "                                      display_labels=[x for x in range(1,7)])\n",
    "        disp.plot()\n",
    "        plt.show()\n",
    "        print(f'F1 score across fold: {f1_fold}')\n",
    "        print(f'Cohen kappa score across fold: {kappa_fold}')\n",
    "        i+=1\n",
    "    \n",
    "    mean_f1_score = np.mean(f1_scores)\n",
    "    mean_kappa_score = np.mean(kappa_scores)\n",
    "else:\n",
    "    n_splits = 15\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=0)\n",
    "\n",
    "    f1_scores = []\n",
    "    kappa_scores = []\n",
    "    models = []\n",
    "    predictions = []\n",
    "    callbacks = [log_evaluation(period=25), early_stopping(stopping_rounds=75,first_metric_only=True)]\n",
    "\n",
    "    i=1\n",
    "    for train_index, test_index in skf.split(X, y_split):\n",
    "\n",
    "        print('fold',i)\n",
    "        X_train_fold, X_test_fold = X[train_index], X[test_index]\n",
    "\n",
    "\n",
    "        y_train_fold, y_test_fold, y_test_fold_int = y[train_index], y[test_index], y_split[test_index]\n",
    "\n",
    "        model = lgb.LGBMRegressor(\n",
    "                    objective = qwk_obj,\n",
    "                    metrics = 'None',\n",
    "                    learning_rate = 0.05,\n",
    "                    max_depth = 5,\n",
    "                    num_leaves = 10,\n",
    "                    colsample_bytree=0.3,\n",
    "                    reg_alpha = 0.7,\n",
    "                    reg_lambda = 0.1,\n",
    "                    n_estimators=700,\n",
    "                    random_state=42,\n",
    "                    extra_trees=True,\n",
    "                    class_weight='balanced',\n",
    "                    verbosity = - 1)\n",
    "\n",
    "        predictor = model.fit(X_train_fold,\n",
    "                                      y_train_fold,\n",
    "                                      eval_names=['train', 'valid'],\n",
    "                                      eval_set=[(X_train_fold, y_train_fold), (X_test_fold, y_test_fold)],\n",
    "                                      eval_metric=quadratic_weighted_kappa,\n",
    "                                      callbacks=callbacks,)\n",
    "        models.append(predictor)\n",
    "        predictions_fold = predictor.predict(X_test_fold)\n",
    "        predictions_fold = predictions_fold + a\n",
    "        oof[test_index]=predictions_fold\n",
    "        predictions_fold = predictions_fold.clip(1, 6).round()\n",
    "        predictions.append(predictions_fold)\n",
    "        f1_fold = f1_score(y_test_fold_int, predictions_fold, average='weighted')\n",
    "        f1_scores.append(f1_fold)\n",
    "\n",
    "\n",
    "        kappa_fold = cohen_kappa_score(y_test_fold_int, predictions_fold, weights='quadratic')\n",
    "        kappa_scores.append(kappa_fold)\n",
    "\n",
    "        cm = confusion_matrix(y_test_fold_int, predictions_fold, labels=[x for x in range(1,7)])\n",
    "\n",
    "        disp = ConfusionMatrixDisplay(confusion_matrix=cm,\n",
    "                                      display_labels=[x for x in range(1,7)])\n",
    "        disp.plot()\n",
    "        plt.show()\n",
    "        print(f'F1 score across fold: {f1_fold}')\n",
    "        print(f'Cohen kappa score across fold: {kappa_fold}')\n",
    "        i+=1"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-27T07:09:55.633716Z",
     "iopub.execute_input": "2024-06-27T07:09:55.634409Z",
     "iopub.status.idle": "2024-06-27T07:46:29.92728Z",
     "shell.execute_reply.started": "2024-06-27T07:09:55.634376Z",
     "shell.execute_reply": "2024-06-27T07:46:29.926329Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "mean_f1_score = np.mean(f1_scores)\n",
    "mean_kappa_score = np.mean(kappa_scores)\n",
    "\n",
    "print(\"=\"*50)\n",
    "print(f'Mean F1 score across {n_splits} folds: {mean_f1_score}')\n",
    "print(f'Mean Cohen kappa score across {n_splits} folds: {mean_kappa_score}')\n",
    "print(\"=\"*50)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-27T07:46:29.928622Z",
     "iopub.execute_input": "2024-06-27T07:46:29.928935Z",
     "iopub.status.idle": "2024-06-27T07:46:29.93455Z",
     "shell.execute_reply.started": "2024-06-27T07:46:29.92891Z",
     "shell.execute_reply": "2024-06-27T07:46:29.933699Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "oof"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-27T07:46:29.935568Z",
     "iopub.execute_input": "2024-06-27T07:46:29.935903Z",
     "iopub.status.idle": "2024-06-27T07:46:29.95626Z",
     "shell.execute_reply.started": "2024-06-27T07:46:29.935875Z",
     "shell.execute_reply": "2024-06-27T07:46:29.955418Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# **threshold**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "def find_thresholds(true, pred, steps=50):\n",
    "\n",
    "    # SAVE TRIALS FOR PLOTTING\n",
    "    xs = [[],[],[],[],[]]\n",
    "    ys = [[],[],[],[],[]]\n",
    "\n",
    "    # COMPUTE BASELINE METRIC\n",
    "    threshold = [1.5, 2.5, 3.5, 4.5, 5.5]\n",
    "    pred2 = pd.cut(pred, [-np.inf] + threshold + [np.inf], \n",
    "                    labels=[1,2,3,4,5,6]).astype('int32')\n",
    "    best = cohen_kappa_score(true, pred2, weights=\"quadratic\")\n",
    "    # cohen_kappa_score(y_test_fold_int, predictions_fold, weights='quadratic')\n",
    "\n",
    "    # FIND FIVE OPTIMAL THRESHOLDS\n",
    "    for k in range(5):\n",
    "        for sign in [1,-1]:\n",
    "            v = threshold[k]\n",
    "            threshold2 = threshold.copy()\n",
    "            stop = 0\n",
    "            while stop<steps:\n",
    "\n",
    "                # TRY NEW THRESHOLD\n",
    "                v += sign * 0.001\n",
    "                threshold2[k] = v\n",
    "                pred2 = pd.cut(pred, [-np.inf] + threshold2 + [np.inf], \n",
    "                                labels=[1,2,3,4,5,6]).astype('int32')\n",
    "                metric = cohen_kappa_score(true, pred2, weights=\"quadratic\")\n",
    "\n",
    "                # SAVE TRIALS FOR PLOTTING\n",
    "                xs[k].append(v)\n",
    "                ys[k].append(metric)\n",
    "\n",
    "                # EARLY STOPPING\n",
    "                if metric<=best:\n",
    "                    stop += 1\n",
    "                else:\n",
    "                    stop = 0\n",
    "                    best = metric\n",
    "                    threshold = threshold2.copy()\n",
    "\n",
    "    # COMPUTE FINAL METRIC\n",
    "    pred2 = pd.cut(pred, [-np.inf] + threshold + [np.inf], \n",
    "                    labels=[1,2,3,4,5,6]).astype('int32')\n",
    "    best = cohen_kappa_score(true, pred2, weights=\"quadratic\")   \n",
    "\n",
    "    # RETURN RESULTS\n",
    "    threshold = [np.round(t,3) for t in threshold]\n",
    "    return best, threshold, xs, ys\n",
    "\n",
    "if threshold:\n",
    "    best, thresholds, xs, ys = find_thresholds(x_score_thres, oof, steps=500)\n",
    "    print('Best thresholds are:', thresholds )\n",
    "    print('=> achieve Overall CV QWK score =', best )"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-27T07:46:29.957364Z",
     "iopub.execute_input": "2024-06-27T07:46:29.957665Z",
     "iopub.status.idle": "2024-06-27T07:46:29.971949Z",
     "shell.execute_reply.started": "2024-06-27T07:46:29.957642Z",
     "shell.execute_reply": "2024-06-27T07:46:29.971031Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# import pickle\n",
    "\n",
    "# with open('models.pkl', 'wb') as f:\n",
    "#     pickle.dump(models, f)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-27T07:46:29.973113Z",
     "iopub.execute_input": "2024-06-27T07:46:29.973689Z",
     "iopub.status.idle": "2024-06-27T07:46:29.98373Z",
     "shell.execute_reply.started": "2024-06-27T07:46:29.973657Z",
     "shell.execute_reply": "2024-06-27T07:46:29.982887Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# with open('models.pkl', 'rb') as f:\n",
    "#     models = pickle.load(f)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-27T07:46:29.984867Z",
     "iopub.execute_input": "2024-06-27T07:46:29.985201Z",
     "iopub.status.idle": "2024-06-27T07:46:29.993272Z",
     "shell.execute_reply.started": "2024-06-27T07:46:29.985166Z",
     "shell.execute_reply": "2024-06-27T07:46:29.992385Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# **Inference**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# Paragraph\n",
    "tmp = Paragraph_Preprocess(test)\n",
    "test_feats = Paragraph_Eng(tmp)\n",
    "# Sentence\n",
    "tmp = Sentence_Preprocess(test)\n",
    "test_feats = test_feats.merge(Sentence_Eng(tmp), on='essay_id', how='left')\n",
    "# Word\n",
    "tmp = Word_Preprocess(test)\n",
    "test_feats = test_feats.merge(Word_Eng(tmp), on='essay_id', how='left')\n",
    "\n",
    "# Tfidf\n",
    "test_tfid = vectorizer.transform([i for i in test['full_text']])\n",
    "dense_matrix = test_tfid.toarray()\n",
    "df = pd.DataFrame(dense_matrix)\n",
    "tfid_columns = [ f'tfid_{i}' for i in range(len(df.columns))]\n",
    "df.columns = tfid_columns\n",
    "df['essay_id'] = test_feats['essay_id']\n",
    "test_feats = test_feats.merge(df, on='essay_id', how='left')\n",
    "\n",
    "# CountVectorizer\n",
    "test_tfid = vectorizer_cnt.transform([i for i in test['full_text']])\n",
    "dense_matrix = test_tfid.toarray()\n",
    "df = pd.DataFrame(dense_matrix)\n",
    "tfid_columns = [ f'tfid_cnt_{i}' for i in range(len(df.columns))]\n",
    "df.columns = tfid_columns\n",
    "df['essay_id'] = test_feats['essay_id']\n",
    "test_feats = test_feats.merge(df, on='essay_id', how='left')\n",
    "\n",
    "if six_features:\n",
    "    test_feats = pd.concat((test_feats, test_feedback_predictions_df), axis=1)\n",
    "if gkf_as_feature:\n",
    "    for i in range(7):\n",
    "        test_feats[f'prompt_{i}'] = test_prompts_df[f'prompt_{i}']\n",
    "\n",
    "for i in range(6):\n",
    "    test_feats[f'deberta_oof_{i}'] = predicted_score[:, i]\n",
    "\n",
    "# Features number\n",
    "feature_names = list(filter(lambda x: x not in ['essay_id','score'], test_feats.columns))\n",
    "print('Features number: ',len(feature_names))\n",
    "test_feats.head(3)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-27T07:46:29.994358Z",
     "iopub.execute_input": "2024-06-27T07:46:29.996084Z",
     "iopub.status.idle": "2024-06-27T07:46:30.534523Z",
     "shell.execute_reply.started": "2024-06-27T07:46:29.996054Z",
     "shell.execute_reply": "2024-06-27T07:46:30.53357Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# **Gkf**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "import plotly.express as px\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, Trainer, TrainingArguments, EvalPrediction\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, ConfusionMatrixDisplay"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-27T07:46:30.53575Z",
     "iopub.execute_input": "2024-06-27T07:46:30.536068Z",
     "iopub.status.idle": "2024-06-27T07:46:31.090819Z",
     "shell.execute_reply.started": "2024-06-27T07:46:30.536043Z",
     "shell.execute_reply": "2024-06-27T07:46:31.090071Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "if gkf:\n",
    "    gkf_train_df = pd.read_csv(\"/kaggle/input/learning-agency-lab-automated-essay-scoring-2/train.csv\")\n",
    "    # gkf_test_df = pd.read_csv(\"/kaggle/input/learning-agency-lab-automated-essay-scoring-2/test.csv\")\n",
    "    gkf_test_df = pd.read_csv(\"/kaggle/input/learning-agency-lab-automated-essay-scoring-2/test.csv\")\n",
    "    persuade = pd.read_csv(\"/kaggle/input/persaude-corpus-2/persuade_2.0_human_scores_demo_id_github.csv\")\n",
    "    \n",
    "    intersection = pd.merge(gkf_train_df, persuade, on=\"full_text\", how=\"inner\")[[\"essay_id\", \"full_text\", \"prompt_name\"]].reset_index(drop=True)\n",
    "    difference = gkf_train_df[~gkf_train_df[\"essay_id\"].isin(intersection[\"essay_id\"])].reset_index(drop=True)\n",
    "    print(\"len(intersection):\", len(intersection))\n",
    "    print(\"len(difference):\", len(difference))\n",
    "    \n",
    "    intersection[\"label\"] =  intersection[\"prompt_name\"].astype(\"category\").cat.codes\n",
    "    intersection.head()\n",
    "    \n",
    "    prompt_to_id = intersection.drop_duplicates(subset=(\"prompt_name\", \"label\"))[[\"prompt_name\", \"label\"]]\n",
    "    label2id = {row[\"prompt_name\"]: row[\"label\"] for _, row in prompt_to_id.iterrows()}\n",
    "    id2label = {row[\"label\"]: row[\"prompt_name\"] for _, row in prompt_to_id.iterrows()}\n",
    "    prompt_to_id\n",
    "    \n",
    "    intersection = pd.merge(gkf_test_df, persuade, on=\"full_text\", how=\"inner\")[[\"essay_id\", \"full_text\", \"prompt_name\"]].reset_index(drop=True)\n",
    "    difference = gkf_test_df[~gkf_test_df[\"essay_id\"].isin(intersection[\"essay_id\"])].reset_index(drop=True)\n",
    "    print(\"len(intersection):\", len(intersection))\n",
    "    print(\"len(difference):\", len(difference))\n",
    "    \n",
    "    intersection_ds = Dataset.from_pandas(intersection)\n",
    "    diff_ds = Dataset.from_pandas(difference)\n",
    "    intersection_ds = intersection_ds.map(lambda i: tokenizer(i[\"full_text\"], max_length=1024, truncation=True), batched=True)\n",
    "    diff_ds = diff_ds.map(lambda i: tokenizer(i[\"full_text\"], max_length=1024, truncation=True), batched=True)\n",
    "    \n",
    "    MODEL_PATHS_PROMPT = [\n",
    "        '/kaggle/input/aes-train-prompt-added-csv/output/*/*'\n",
    "    ]\n",
    "    EVAL_BATCH_SIZE = 1\n",
    "\n",
    "    model_pro = []\n",
    "    for path in MODEL_PATHS_PROMPT:\n",
    "        model_pro.extend(glob(path))\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_pro[0])\n",
    "\n",
    "    def tokenize(sample):\n",
    "        return tokenizer(sample['full_text'], max_length=MAX_LENGTH, truncation=True)\n",
    "\n",
    "    df_test = pd.read_csv(TEST_DATA_PATH)\n",
    "    ds = Dataset.from_pandas(df_test).map(tokenize).remove_columns(['essay_id', 'full_text'])\n",
    "\n",
    "    args = TrainingArguments(\n",
    "        \".\", \n",
    "        per_device_eval_batch_size=EVAL_BATCH_SIZE, \n",
    "        report_to=\"none\"\n",
    "    )\n",
    "    \n",
    "    if len(difference)!=0:\n",
    "        predictions = []\n",
    "        for model in model_pro:\n",
    "            model = AutoModelForSequenceClassification.from_pretrained(model)\n",
    "            trainer = Trainer(\n",
    "                model=model, \n",
    "                args=args, \n",
    "                data_collator=DataCollatorWithPadding(tokenizer), \n",
    "                tokenizer=tokenizer\n",
    "            )    \n",
    "            preds = trainer.predict(diff_ds).predictions\n",
    "            predictions.append(softmax(preds, axis=-1))\n",
    "            del model, trainer\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "\n",
    "        predicted_score = 0.\n",
    "        for p in predictions:\n",
    "            predicted_score += p\n",
    "\n",
    "        predicted_score /= len(predictions)\n",
    "\n",
    "        print(\"-------predicted_score--------:\")\n",
    "        print(predicted_score)\n",
    "        \n",
    "    if gkf_smooth:\n",
    "        if len(difference)!=0:\n",
    "            predictions=np.array(predicted_score)\n",
    "            predicted_score=dict()\n",
    "            for i in range(7):\n",
    "                predicted_score[f'prompt_{i}'] = predictions[:, i]\n",
    "\n",
    "            essay_id = diff_ds[\"essay_id\"]\n",
    "            result_df = pd.DataFrame({\"essay_id\": essay_id})\n",
    "\n",
    "            for i in range(7):\n",
    "                result_df[f\"prompt_{i}\"] = predicted_score[f\"prompt_{i}\"]\n",
    "            print(\"-------result_df--------:\")\n",
    "            print(result_df)\n",
    "        \n",
    "        if len(intersection)!=0:\n",
    "            prompt_id = [label2id[i] for i in intersection[\"prompt_name\"]]\n",
    "            intersection['prompt_id']=prompt_id\n",
    "\n",
    "            intersection_df = intersection[[\"essay_id\"]].copy()\n",
    "            for i in range(7):\n",
    "                if i==int(intersection['prompt_id']):\n",
    "                    intersection_df[f\"prompt_{i}\"] = 1\n",
    "                else:\n",
    "                    intersection_df[f\"prompt_{i}\"] = 0\n",
    "            print(\"-------intersection_df--------:\")\n",
    "            print(intersection_df)\n",
    "        \n",
    "        if len(difference)!=0 and len(intersection)!=0:\n",
    "            final_df = pd.concat([result_df, intersection_df])\n",
    "        elif len(difference)!=0 and len(intersection)==0:\n",
    "            final_df=result_df\n",
    "        else:\n",
    "            final_df=intersection_df\n",
    "        print(\"-------final_df--------:\")\n",
    "        print(final_df)\n",
    "        prompts_df=pd.merge(df_test,final_df,on=\"essay_id\")\n",
    "        print(\"-------prompts_df--------:\")\n",
    "        print(prompts_df)\n",
    "    else:\n",
    "        predicted_score = 0.\n",
    "        for p in predictions:\n",
    "            predicted_score += p\n",
    "\n",
    "        predicted_score /= len(predictions)\n",
    "        predicted_score\n",
    "        y_pred=predicted_score.argmax(-1)\n",
    "        y_pred\n",
    "\n",
    "        essay_id = diff_ds[\"essay_id\"]\n",
    "        prompt_name = [id2label[i] for i in predicted_score.argmax(-1)]  # convert prompt id back to prompt name\n",
    "        result_df = pd.DataFrame({\"essay_id\": essay_id, \"prompt_name\": prompt_name, \"predicted\": [True] * len(essay_id)})\n",
    "        result_df\n",
    "        intersection_df = intersection[[\"essay_id\", \"prompt_name\"]].copy()\n",
    "        intersection_df[\"predicted\"] = False\n",
    "        final_df = pd.concat([result_df, intersection_df])\n",
    "        final_df.to_csv(\"predicted_prompt.csv\", index=False)\n",
    "        final_df\n",
    "        prompts_df=pd.merge(df_test,final_df,on=\"essay_id\")\n",
    "        prompts_df\n",
    "        prompt_name = [label2id[i] for i in prompts_df[\"prompt_name\"]]\n",
    "        prompts_df['prompt_id']=prompt_name\n",
    "        print(\"-------prompts_df--------:\")\n",
    "        print(prompts_df)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-27T07:46:31.092067Z",
     "iopub.execute_input": "2024-06-27T07:46:31.092352Z",
     "iopub.status.idle": "2024-06-27T07:46:31.12113Z",
     "shell.execute_reply.started": "2024-06-27T07:46:31.092329Z",
     "shell.execute_reply": "2024-06-27T07:46:31.120231Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "probabilities = []\n",
    "for model in models:\n",
    "    print(model)\n",
    "    proba= model.predict(test_feats[feature_select])+ a\n",
    "    probabilities.append(proba)\n",
    "\n",
    "if gkf_smooth:\n",
    "    prompt_Pro = pd.DataFrame({f\"prompt_{0}\": prompts_df[f\"prompt_{0}\"]})\n",
    "    for i in range(1,7):\n",
    "        prompt_Pro[f\"prompt_{i}\"] = prompts_df[f\"prompt_{i}\"]\n",
    "    prompt_Pro=np.array(prompt_Pro)\n",
    "    probabilities=np.array(probabilities)\n",
    "    probabilities=probabilities.T\n",
    "    predictions=[]\n",
    "    \n",
    "    for i in range(len(prompt_Pro)):\n",
    "        predictions.append(np.dot(prompt_Pro[i],probabilities[i]))\n",
    "    predictions=np.array(predictions)\n",
    "    print(predictions)\n",
    "else:\n",
    "    predictions = np.mean(probabilities, axis=0)\n",
    "    print(predictions)\n",
    "\n",
    "\n",
    "if threshold:\n",
    "    predictions = pd.cut(predictions, [-np.inf] + thresholds + [np.inf], \n",
    "                       labels=[1,2,3,4,5,6]).astype('int32')  # add\n",
    "else:\n",
    "    predictions = np.round(predictions.clip(1, 6))\n",
    "\n",
    "print(predictions)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-27T07:46:31.122577Z",
     "iopub.execute_input": "2024-06-27T07:46:31.122969Z",
     "iopub.status.idle": "2024-06-27T07:46:31.961983Z",
     "shell.execute_reply.started": "2024-06-27T07:46:31.122938Z",
     "shell.execute_reply": "2024-06-27T07:46:31.96075Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "submission=pd.read_csv(\"/kaggle/input/learning-agency-lab-automated-essay-scoring-2/sample_submission.csv\")\n",
    "submission['score']=predictions\n",
    "submission['score']=submission['score'].astype(int)\n",
    "submission.to_csv(\"submission.csv\",index=None)\n",
    "display(submission.head())"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-27T07:46:31.963194Z",
     "iopub.execute_input": "2024-06-27T07:46:31.963512Z",
     "iopub.status.idle": "2024-06-27T07:46:31.987644Z",
     "shell.execute_reply.started": "2024-06-27T07:46:31.963484Z",
     "shell.execute_reply": "2024-06-27T07:46:31.986862Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ]
}
